{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=78):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./Train\"\n",
    "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(dataset))\n",
    "val_size = int(0.075 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNNModel(num_classes=len(dataset.classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1700.0440, Accuracy: 50.68%\n",
      "Epoch 2/10, Loss: 743.7489, Accuracy: 76.02%\n",
      "Epoch 3/10, Loss: 578.7818, Accuracy: 80.97%\n",
      "Epoch 4/10, Loss: 496.9485, Accuracy: 83.62%\n",
      "Epoch 5/10, Loss: 455.7138, Accuracy: 84.95%\n",
      "Epoch 6/10, Loss: 425.9998, Accuracy: 85.70%\n",
      "Epoch 7/10, Loss: 402.9836, Accuracy: 86.65%\n",
      "Epoch 8/10, Loss: 377.7350, Accuracy: 87.32%\n",
      "Epoch 9/10, Loss: 364.0753, Accuracy: 87.73%\n",
      "Epoch 10/10, Loss: 355.4467, Accuracy: 88.12%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"armenian_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.16%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "Predicted Armenian letter: Բ\n"
     ]
    }
   ],
   "source": [
    "def test_single_image(image_path, model, device):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),  \n",
    "        transforms.Resize((64, 64)),  \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.5,), (0.5,))  \n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)\n",
    "\n",
    "    image = image.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted_class = torch.max(output, 1)\n",
    "\n",
    "    armenian_dict = {\n",
    "        0: \"Ա\", 1: \"Բ\", 2: \"Գ\", 3: \"Դ\", 4: \"Ե\", 5: \"Զ\", 6: \"Է\", 7: \"Ը\", 8: \"Թ\", 9: \"Ժ\",\n",
    "        10: \"Ի\", 11: \"Լ\", 12: \"Խ\", 13: \"Ծ\", 14: \"Կ\", 15: \"Հ\", 16: \"Ձ\", 17: \"Ղ\", 18: \"Ճ\",\n",
    "        19: \"Մ\", 20: \"Յ\", 21: \"Ն\", 22: \"Շ\", 23: \"Ո\", 24: \"Ու\", 25: \"Չ\", 26: \"Պ\", 27: \"Ջ\",\n",
    "        28: \"Ռ\", 29: \"Ս\", 30: \"Վ\", 31: \"Տ\", 32: \"Ր\", 33: \"Ց\", 34: \"Փ\", 35: \"Ք\", 36: \"Եվ\",\n",
    "        37: \"Օ\", 38: \"Ֆ\", 39: \"ա\", 40: \"բ\", 41: \"գ\", 42: \"դ\", 43: \"ե\", 44: \"զ\", 45: \"է\",\n",
    "        46: \"ը\", 47: \"թ\", 48: \"ժ\", 49: \"ի\", 50: \"լ\", 51: \"խ\", 52: \"ծ\", 53: \"կ\", 54: \"հ\",\n",
    "        55: \"ձ\", 56: \"ղ\", 57: \"ճ\", 58: \"մ\", 59: \"յ\", 60: \"ն\", 61: \"շ\", 62: \"ո\", 63: \"ու\",\n",
    "        64: \"չ\", 65: \"պ\", 66: \"ջ\", 67: \"ռ\", 68: \"ս\", 69: \"վ\", 70: \"տ\", 71: \"ր\", 72: \"ց\",\n",
    "        73: \"փ\", 74: \"ք\", 75: \"և\", 76: \"օ\", 77: \"ֆ\"\n",
    "    }\n",
    "\n",
    "    predicted_label = armenian_dict[predicted_class.item()]\n",
    "    print(f\"Predicted class: {predicted_class.item()}\")\n",
    "    print(f\"Predicted Armenian letter: {predicted_label}\")\n",
    "\n",
    "\n",
    "image_path = \"3.jpeg\"\n",
    "test_single_image(image_path, model, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
